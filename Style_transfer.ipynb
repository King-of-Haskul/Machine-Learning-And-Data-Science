{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_transfer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZMUU5pVTEYW4b5wlEIv2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/King-of-Haskul/Machine-Learning-And-Data-Science/blob/main/Style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md5qvI6pSbuq"
      },
      "source": [
        "##Neurat style transfer\n",
        "General process:\n",
        "1. Set up a network that computes VGG19 layer activations for the style-refernce image, the target image, and the generated image at the same time.\n",
        "\n",
        "2. Use the layer activations computed over these three images to define the loss function described earlier, which you'll minimize in order to achieve style transfer.\n",
        "\n",
        "3. Set up a gradient-descent process to minimize this loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "brzDJwLRSVAO",
        "outputId": "83a72fb1-4800-4a88-ce7c-f70b403d7e4c"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-52c49b62-21e4-4788-9f2b-3e6d8a32204e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-52c49b62-21e4-4788-9f2b-3e6d8a32204e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving pic_2.jpg to pic_2.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj__grl3Il8t"
      },
      "source": [
        "#Defining initial variables\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "target_image_path = '/content/pic_2.jpg'  #Path to the image we want to transform\n",
        "style_reference_image_path = '/content/abstract.jpg' #Path to the style image\n",
        "\n",
        "width, height = load_img(target_image_path).size\n",
        "img_height = 400  #Dimensions of the generated image\n",
        "img_width = int(width * img_height / height)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ZJMW7dWXAp"
      },
      "source": [
        "#Auxiliary functions\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import vgg19\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  img = load_img(image_path, target_size=(img_height, img_width))\n",
        "  img = img_to_array(img)\n",
        "  img = np.expand_dims(img, axis = 0)\n",
        "  img = vgg19.preprocess_input(img)\n",
        "  return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "  #Zero-centering by removing the mean pixel value from ImageNet. This reverses a transformation done by vgg19.preprocess_input\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1] #Converts images from 'BGR' to 'RGB', again another reversal of vgg19.preprocess_input\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Tle_IQYWwA",
        "outputId": "ae6da122-e846-4b44-d273-b5356cefd637"
      },
      "source": [
        "#Loading the pretrained VGG19 network and applying it to the tree images\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()\n",
        "\n",
        "target_image = K.constant(preprocess_image(target_image_path))\n",
        "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
        "combination_image = K.placeholder((1, img_height, img_width, 3))  #Placeholder that will contain the generated image\n",
        "\n",
        "input_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0)\n",
        "\n",
        "model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "print('Model loaded.')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSS1g1kMZMBC"
      },
      "source": [
        "#Content Loss: It will make sure the top layer of the VGG19 convnethas a similar view of the target image and the generated image\n",
        "def content_loss(base, combination):\n",
        "  return K.sum(K.square(combination - base))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q0xuPi5Zox6"
      },
      "source": [
        "#Style Loss: Uses an auxiliary function to compute the Gram matrix of an input matrxis i.e. a map of correlations found in the original feature matrix\n",
        "def gram_matrix(x):\n",
        "  features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "  gram = K.dot(features, K.transpose(features)) #inner product of features\n",
        "  return gram\n",
        "\n",
        "def style_loss(style, combination):\n",
        "  S = gram_matrix(style)\n",
        "  C = gram_matrix(combination)\n",
        "  channels = 3\n",
        "  size = img_height * img_width\n",
        "  return K.sum(K.square(S - C)) / (4. * (channels**2) * (size ** 2))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIokCf7Taxcs"
      },
      "source": [
        "#Total Variation loss: It encourages spatial continuity in the generated image, thus avoiding overly pixelated results\n",
        "def total_variation_loss(x):\n",
        "  a = K.square(\n",
        "      x[: , : img_height - 1, : img_width - 1, :] - x[:, 1:, :img_width - 1, :]\n",
        "  )\n",
        "  b = K.square(\n",
        "      x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, :img_width - 1, :]\n",
        "  )\n",
        "  return K.sum(K.pow(a + b, 1.25))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13WrbX3Tbuq_"
      },
      "source": [
        "#Defining the final loss that we have to minimize, which is a weighted average of the above three loses\n",
        "\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "content_layer = 'block5_conv2'  #Layer used for content loss\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] #Layers used for style loss\n",
        "\n",
        "total_variation_weight = 1e-4\n",
        "style_weight = 1.\n",
        "content_weight = 0.025 #Higher content-weight means the target content will be more recognizable in the generated image\n",
        "\n",
        "#Adding the content loss\n",
        "loss = K.variable(0.) #Defining loss by adding all component to this scalar variable\n",
        "layer_features = outputs_dict[content_layer]\n",
        "target_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss = loss + (content_weight * content_loss(target_image_features, combination_features))\n",
        "\n",
        "#Adding the syle loss component for each target layer\n",
        "for layer_name in style_layers:\n",
        "  layer_features = outputs_dict[layer_name]\n",
        "  style_reference_features = layer_features[1, :, :, :]\n",
        "  combination_features = layer_features[2, :, :, :]\n",
        "  sl = style_loss(style_reference_features, combination_features)\n",
        "  loss = loss + ((style_weight / len(style_layers))* sl)\n",
        "\n",
        "#Adding the total variational loss\n",
        "loss = loss + total_variation_weight*total_variation_loss(combination_image)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGEgznNfkZer",
        "outputId": "6e96b91a-7669-45e4-92b9-d196acec6642"
      },
      "source": [
        "combination_image"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Placeholder_231:0' shape=(1, 400, 301, 3) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB_9AsXxeHaH"
      },
      "source": [
        "#Setting up the gradient-descent process\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()\n",
        "\n",
        "grads = K.gradients(loss, combination_image)[0] #Gets the gradients of the generated image with regard to the loss\n",
        "\n",
        "fetch_loss_and_grads = K.function([combination_image], [loss, grads]) #Function to fetch the values of the current loss and the current gradients\n",
        "\n",
        "class Evaluator(object): \n",
        "  #This class wraps fetch_loss_and_grads in a way that lets you retrieve the losses and gradients via two seperate method calls, which is required by SciPy optimizer\n",
        "  def __init__(self):\n",
        "    self.loss_value = None\n",
        "    self.grads_value = None\n",
        "  \n",
        "  def loss(self, x):\n",
        "    assert self.loss_value is None\n",
        "    x = x.reshape((1, img_height, img_width, 3))\n",
        "    outs = fetch_loss_and_grads([x])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1].flatten().astype('float64')\n",
        "    self.loss_value = loss_value\n",
        "    self.grad_values = grad_values #stored as cache\n",
        "    return self.loss_value\n",
        "  \n",
        "  def grads(self, x):\n",
        "    assert self.loss_value is not None\n",
        "    grad_values = np.copy(self.grad_values)\n",
        "    self.loss_value = None\n",
        "    self.grad_values = None #Resetting the object\n",
        "    return grad_values\n",
        "  \n",
        "evaluator = Evaluator()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "iIhgih9cjm2s",
        "outputId": "3e0dbf40-bfc9-4fa6-df84-cb1c40bc9681"
      },
      "source": [
        "#Style-transfer loop using L-BFGS algorithm for optimization\n",
        "\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from imageio import imwrite\n",
        "import time\n",
        "\n",
        "result_prefix = 'my_result'\n",
        "iterations = 20\n",
        "\n",
        "x = preprocess_image(target_image_path)\n",
        "x = x.flatten()\n",
        "\n",
        "for i in range(iterations):\n",
        "  print('Start of iteration', i)\n",
        "  start_time = time.time()\n",
        "  x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, evaluator.grads, maxfun=20)\n",
        "  #Above line runs L-BFGS optimizations over the pixels of the generated image to minimze the neural style loss\n",
        "  print('Current loss value:', min_val)\n",
        "  img = x.copy().reshape((img_height, img_width, 3))\n",
        "  img = deprocess_image(img)\n",
        "  fname = result_prefix + 'at_iteration_%d.png' % i\n",
        "  imwrite(fname, img)\n",
        "  print('Image saved as ', fname)\n",
        "  end_time = time.time()\n",
        "  print('Iteration %d completed in %ds' % (i, end_time - start_time)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor i in range(iterations):\\n  print('Start of iteration', i)\\n  start_time = time.time()\\n  x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, evaluator.grads, maxfun=20)\\n  #Above line runs L-BFGS optimizations over the pixels of the generated image to minimze the neural style loss\\n  print('Current loss value:', min_val)\\n  img = x.copy().reshape((img_height, img_width, 3))\\n  img = deprocess_image(img)\\n  fname = result_prefix + 'at_iteration_%d.png' % i\\n  imwrite(fname, img)\\n  print('Image saved as ', fname)\\n  end_time = time.time()\\n  print('Iteration %d completed in %ds' % (i, end_time - start_time)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7ur_9dUnVxF"
      },
      "source": [
        ""
      ],
      "execution_count": 60,
      "outputs": []
    }
  ]
}